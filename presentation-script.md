## Introduction

Hey everyone, Manny from the GitHub in Microsoft team!

Last week, I was also heads-down during the hackathon but on an "unofficial" project.

While I don't have a name for it, a few of the names generated by ChatGPT were:

-   SPDXinator
-   AI-License Ninja
-   LicenseSlayer 3000

So clearly, my project was around licenses, specifically open-source licenses. I was able to train an AI model to predict SPDX-formatted licenses from open-source license texts. Basically - provide some text, and the model will output the SPDX license ID associated with that text, with extremely high accuracy.

## Problem Statement

Today, Microsoft relies on a open-source tool called ClearlyDefined to identify licenses. ClearlyDefined is a community-led initiative to help clearly define the open source landscape around licenses. It's a great tool, but it's far from perfect. It's great because it houses the largest and most accurate dataset for open-source licenses on the internet. But it's not perfect because the service is expensive, brittle, and arugably miserable to maintain as a developer. Because of this, I created SPDXinator.

## Project Goals and Objectives

The objectives were pretty clear from the start:

-   I wanted to build an AI model that's super accurate at predicting licenses
-   I wanted to leverage the hottest, new open-source framework for building it
-   I wanted to use as little training data as possible
-   I wanted the model to be as small as possible. Small enough to run on free-tier Azure App Service

Sounds a bit optimistic for a solo-hackathon project, right?
Well, I'm happy to say that I was able to accomplish all of these goals thanks to HuggingFace Transformers.

## HuggingFace

I won't go too deep into HuggingFace but basically, it's an open-source framework and community that focuses on NLP and AI. It provides easy-to-use API's and a wide-range of pre-trained models such as BERT, GPT, LLAMA, and more. It's built on top of PyTorch and it's the hottest framework for building AI models today - a whopping 109k stars on GitHub. I encourage you to check it out - https://huggingface.co/.

## The Juice

Lets dive into the juicy stuff.

-   I'll start by showing you snippets of the dataset I used to train the model and how I obtained it.
    -   NPM components
    -   ClearlyDefined, 36M components w/ licenses
    -   134 unique licenses
    -   Scripts to evaluate and clean the data - super iterative.
-   Then I'll show you how I trained the model.
-   Finally, I'll show you a head-to-head comparison between SPDXinator versus ChatGPT-3.5

**_I do want to emphasize that I am not an AI nor a Python expert - I learned both during this hackathon. If you see or hear anything weird,
feel free to let me know during Q&A._**

## Dataset

To keep things simple for the sake of the hackathon, I wanted to focus on a small subset of the license landscape - specifically licenses obtained from NPM components. I chose NPM because it's the most complete component dataset ClearlyDefined has at over 17M NPM components

Those 17M components gave me 134 different unique licenses. I then wrote a few scripts to source the license text from from various unique components. I ended up with a raw dataset of about 11k components.

Let's take a peek at the dataset:

I then used ClearlyDefined to obtain the license text for each component. I then used a script to extract the SPDX license ID from each license text. I ended up with a dataset of 36M components with licenses and 134 unique licenses.

I chose NPM because it's the largest open-source package manager in the world. It's also the package manager that Microsoft uses for all of its open-source projects. I was able to obtain a list of NPM components from the NPM registry. I then used ClearlyDefined to obtain the license text for each component. I then used a script to extract the SPDX license ID from each license text. I ended up with a dataset of 36M components with licenses and 134 unique licenses.

## Notes

-   Show how the model was able to correct clearlydefined's errors with the groupme examples.
-   Imagine how much more accurate we can be if we train the model with more data.
